{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rossmann Store Sales Forecasting Guide with XGBoost\n",
        "\n",
        "This notebook walks through a full, end-to-end workflow for forecasting Rossmann store sales using the gradient boosting library XGBoost. Follow along, execute the cells sequentially, and adapt each section for deeper experimentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to Use this Guide\n",
        "- **Set the stage**: import core analytics libraries and load the Rossmann CSV files.\n",
        "- **Explore and diagnose**: compute summary stats, investigate missing values, and inspect temporal patterns.\n",
        "- **Engineer signal**: enrich the raw timeline with calendar features, competition indicators, and promo flags.\n",
        "- **Model and tune**: assemble a preprocessing + XGBoost pipeline, run randomized hyperparameter tuning, and judge performance.\n",
        "- **Wrap up**: refit the best model on all data and generate test-set forecasts in the competition format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ⚙️ **Prerequisites**: Ensure `xgboost` is available in your environment (`pip install xgboost`). All other libraries ship with typical scientific Python distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 1. Import common scientific Python libraries and utilities.\n",
        "#    Adjust this block if you prefer alternatives (e.g., plotly).\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBRegressor\n",
        "from IPython.display import display\n",
        "\n",
        "pd.options.display.max_columns = 120\n",
        "sns.set(style='whitegrid', context='talk')\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 · Load the Rossmann data files\n",
        "We will work with the Kaggle Rossmann challenge files already present in `../data/rossmann/`. The training data contains historical sales, while `store.csv` adds meta-information such as store type and competition details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 2. Load base CSVs into pandas DataFrames and confirm shapes.\n",
        "#    Using Path keeps the code robust if folders move around.\n",
        "# ============================================================\n",
        "data_dir = Path('../data/rossmann')\n",
        "train_path = data_dir / 'train.csv'\n",
        "test_path = data_dir / 'test.csv'\n",
        "store_path = data_dir / 'store.csv'\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "store_df = pd.read_csv(store_path)\n",
        "\n",
        "print(f'Train shape: {train_df.shape}')\n",
        "print(f'Test shape:  {test_df.shape}')\n",
        "print(f'Store shape: {store_df.shape}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 · Quick first look\n",
        "Use rich table displays to sanity-check the import and familiarize yourself with available columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 3. Peek at the head of each dataframe to inspect fields.\n",
        "#    `display` keeps notebooks readable compared to raw print.\n",
        "# ============================================================\n",
        "display(train_df.head())\n",
        "display(store_df.head())\n",
        "display(test_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 · Baseline diagnostics\n",
        "We want to check column data types and identify potential data quality issues (nulls, string representations of categories, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 4. Summarize schema information: dtypes + non-null counts.\n",
        "#    `.info()` prints directly; no need to wrap in display().\n",
        "# ============================================================\n",
        "train_df.info()\n",
        "print('\n",
        "' + '-' * 80 + '\n",
        "')\n",
        "store_df.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4 · Explore missing values\n",
        "A quick percentage table points us toward the columns that need special handling before modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 5. Compute missing-value ratios to guide data cleaning.\n",
        "#    Sorting helps surface the most problematic columns first.\n",
        "# ============================================================\n",
        "def missing_report(df, name):\n",
        "    pct_missing = df.isna().mean().mul(100).sort_values(ascending=False)\n",
        "    summary = pct_missing[pct_missing > 0].to_frame(name).round(2)\n",
        "    if summary.empty:\n",
        "        print(f'No missing data detected in {name}.')\n",
        "    else:\n",
        "        display(summary)\n",
        "\n",
        "missing_report(train_df, 'train_missing_%')\n",
        "missing_report(store_df, 'store_missing_%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5 · Understand the target (Sales)\n",
        "Sales is right-skewed; inspecting histograms in both raw and log space usually informs loss functions and transforms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 6. Visualize sales distribution (raw vs. log1p) to evaluate\n",
        "#    the extent of skewness and potential benefits of log-scaling.\n",
        "# ============================================================\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
        "sns.histplot(train_df['Sales'], bins=50, ax=axes[0], color='steelblue')\n",
        "axes[0].set_title('Sales Distribution (Raw)')\n",
        "axes[0].set_xlabel('Sales')\n",
        "\n",
        "sns.histplot(np.log1p(train_df['Sales']), bins=50, ax=axes[1], color='darkorange')\n",
        "axes[1].set_title('Sales Distribution (log1p transformed)')\n",
        "axes[1].set_xlabel('log1p(Sales)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6 · Time-series intuition\n",
        "Aggregating total sales over time reveals macro trends, seasonality, and anomalies (e.g., holidays)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 7. Plot aggregated daily sales to spot level shifts or\n",
        "#    promotions affecting all stores. Grouping smooths noise.\n",
        "# ============================================================\n",
        "train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "daily_sales = train_df.groupby('Date')['Sales'].sum()\n",
        "daily_sales.plot(title='Aggregate Daily Sales Across Stores')\n",
        "plt.ylabel('Total Sales')\n",
        "plt.xlabel('Date')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7 · Merge store metadata\n",
        "Before heavy feature engineering, join `store.csv` so that each record carries competition, assortment, and promo information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 8. Merge store-level attributes into train/test copies\n",
        "#    for richer exploratory plots and feature creation.\n",
        "# ============================================================\n",
        "train_merged = train_df.merge(store_df, on='Store', how='left')\n",
        "test_merged = test_df.merge(store_df, on='Store', how='left')\n",
        "\n",
        "print(train_merged.shape, test_merged.shape)\n",
        "display(train_merged.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8 · Feature engineering helper\n",
        "To keep transformations consistent between train and test sets, we will build a single function that applies all cleaning and enrichment steps. The function copies inputs to avoid modifying the original DataFrames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 9. Define a reusable preparation function. Each step is\n",
        "#    commented so you can tweak or extend the feature logic.\n",
        "# ============================================================\n",
        "def prepare_rossmann(raw_df: pd.DataFrame, store_meta: pd.DataFrame, *, is_train: bool) -> pd.DataFrame:\n",
        "    # Work on a copy to avoid side effects when experimenting.\n",
        "    df = raw_df.copy()\n",
        "\n",
        "    # Merge store metadata and parse dates early for reuse.\n",
        "    df = df.merge(store_meta, on='Store', how='left')\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "    # Canonicalize categorical codes to descriptive labels.\n",
        "    state_map = {'0': 'None', 'a': 'PublicHoliday', 'b': 'EasterHoliday', 'c': 'Christmas'}\n",
        "    df['StateHoliday'] = df['StateHoliday'].astype(str).replace(state_map)\n",
        "\n",
        "    # Fill straightforward numeric gaps with sensible defaults.\n",
        "    df['Open'] = df['Open'].fillna(1).astype(int)\n",
        "    df['Promo'] = df['Promo'].fillna(0).astype(int)\n",
        "    df['SchoolHoliday'] = df['SchoolHoliday'].fillna(0).astype(int)\n",
        "    df['Promo2'] = df['Promo2'].fillna(0).astype(int)\n",
        "    df['CompetitionDistance'] = df['CompetitionDistance'].fillna(df['CompetitionDistance'].median())\n",
        "\n",
        "    # When competition/promo start dates are missing, assume\n",
        "    # they became active at the current record's date.\n",
        "    df['CompetitionOpenSinceYear'] = df['CompetitionOpenSinceYear'].fillna(df['Date'].dt.year)\n",
        "    df['CompetitionOpenSinceMonth'] = df['CompetitionOpenSinceMonth'].fillna(df['Date'].dt.month)\n",
        "    df['Promo2SinceYear'] = df['Promo2SinceYear'].fillna(df['Date'].dt.year)\n",
        "    df['Promo2SinceWeek'] = df['Promo2SinceWeek'].fillna(df['Date'].dt.isocalendar().week.astype(int))\n",
        "\n",
        "    # Calendar decompositions unlock seasonal signal for the model.\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Day'] = df['Date'].dt.day\n",
        "    df['WeekOfYear'] = df['Date'].dt.isocalendar().week.astype(int)\n",
        "    df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "    df['IsWeekend'] = df['DayOfWeek'].isin([6, 7]).astype(int)\n",
        "\n",
        "    # Promo interval strings signal recurring campaigns.\n",
        "    df['PromoInterval'] = df['PromoInterval'].fillna('None')\n",
        "    df['MonthStr'] = df['Date'].dt.strftime('%b')\n",
        "    df['IsPromoMonth'] = df.apply(\n",
        "        lambda row: int(row['PromoInterval'] != 'None' and row['MonthStr'] in row['PromoInterval'].split(',')), axis=1\n",
        "    )\n",
        "\n",
        "    # Competition and promo recency features capture duration effects.\n",
        "    df['CompetitionMonthsOpen'] = (\n",
        "        (df['Year'] - df['CompetitionOpenSinceYear']) * 12\n",
        "        + (df['Month'] - df['CompetitionOpenSinceMonth'])\n",
        "    ).clip(lower=0)\n",
        "    df['Promo2WeeksActive'] = (\n",
        "        (df['Year'] - df['Promo2SinceYear']) * 52\n",
        "        + (df['WeekOfYear'] - df['Promo2SinceWeek'])\n",
        "    ).clip(lower=0)\n",
        "\n",
        "    # Stabilize distance by log-scaling; large ranges can slow learning.\n",
        "    df['CompetitionDistanceLog'] = np.log1p(df['CompetitionDistance'])\n",
        "\n",
        "    # Guard against potential negative or null durations.\n",
        "    df['CompetitionMonthsOpen'] = df['CompetitionMonthsOpen'].fillna(0)\n",
        "    df['Promo2WeeksActive'] = df['Promo2WeeksActive'].fillna(0)\n",
        "\n",
        "    # Drop columns not available at prediction time or redundant after engineering.\n",
        "    df = df.drop(columns=['Customers'], errors='ignore')\n",
        "    df = df.drop(columns=['MonthStr'], errors='ignore')\n",
        "\n",
        "    # Create a log-transformed sales column for regression stability.\n",
        "    if is_train:\n",
        "        df = df[df['Sales'] > 0]  # Closed days already have zero sales.\n",
        "        df['Sales_Log'] = np.log1p(df['Sales'])\n",
        "    else:\n",
        "        df['Sales'] = np.nan\n",
        "        df['Sales_Log'] = np.nan\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9 · Apply the feature pipeline\n",
        "Generate processed training and test tables. A quick shape + head check confirms that the function behaved as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 10. Run the preparation helper on train/test sets.\n",
        "#     Keep the original frames intact for reference/EDA.\n",
        "# ============================================================\n",
        "train_ready = prepare_rossmann(train_df, store_df, is_train=True)\n",
        "test_ready = prepare_rossmann(test_df, store_df, is_train=False)\n",
        "\n",
        "print(train_ready.shape, test_ready.shape)\n",
        "display(train_ready.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10 · Create a time-aware validation split\n",
        "Rossmann data is temporal, so we hold out the most recent six weeks as a validation set. This better mimics real forecasting than a random split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 11. Split chronologically: everything before the cutoff is\n",
        "#     training data, the tail segment becomes validation.\n",
        "# ============================================================\n",
        "train_ready = train_ready.sort_values('Date')\n",
        "split_date = train_ready['Date'].max() - pd.Timedelta(weeks=6)\n",
        "train_mask = train_ready['Date'] < split_date\n",
        "\n",
        "feature_drop = ['Sales', 'Sales_Log', 'Date']\n",
        "X_train = train_ready.loc[train_mask].drop(columns=feature_drop + ['Id'], errors='ignore')\n",
        "y_train = train_ready.loc[train_mask, 'Sales_Log']\n",
        "X_val = train_ready.loc[~train_mask].drop(columns=feature_drop + ['Id'], errors='ignore')\n",
        "y_val = train_ready.loc[~train_mask, 'Sales_Log']\n",
        "\n",
        "print(f'Training rows: {X_train.shape[0]}')\n",
        "print(f'Validation rows: {X_val.shape[0]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11 · Build preprocessing + model pipeline\n",
        "Leverage `ColumnTransformer` to one-hot encode categorical variables while scaling numeric ones. The pipeline keeps feature handling consistent during cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 12. Identify categorical vs numeric columns dynamically so\n",
        "#     the pipeline adapts if you add/remove engineered fields.\n",
        "# ============================================================\n",
        "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "numeric_cols = X_train.columns.difference(categorical_cols).tolist()\n",
        "\n",
        "print('Categorical columns:', categorical_cols)\n",
        "print('Numeric columns:', numeric_cols[:10], '...')\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols),\n",
        "    ]\n",
        ")\n",
        "\n",
        "xgb_reg = XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    eval_metric='rmse',\n",
        "    tree_method='hist',\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_estimators=400,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=1.0,\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(\n",
        "    steps=[('preprocess', preprocess), ('model', xgb_reg)]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12 · Hyperparameter tuning\n",
        "We run a randomized search over a compact grid. Feel free to widen the ranges or increase `n_iter` once the workflow is solid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 13. Set up randomized search for XGBoost hyperparameters.\n",
        "#     RMSE on the log-scaled target is the optimization metric.\n",
        "# ============================================================\n",
        "param_distributions = {\n",
        "    'model__n_estimators': [200, 400, 600, 800],\n",
        "    'model__max_depth': [3, 4, 5, 6, 8],\n",
        "    'model__learning_rate': [0.03, 0.05, 0.1, 0.2],\n",
        "    'model__subsample': [0.6, 0.8, 1.0],\n",
        "    'model__colsample_bytree': [0.6, 0.8, 1.0],\n",
        "    'model__min_child_weight': [1, 3, 5, 7],\n",
        "    'model__gamma': [0, 0.05, 0.1, 0.3],\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,\n",
        "    scoring='neg_root_mean_squared_error',\n",
        "    cv=3,\n",
        "    verbose=2,\n",
        "    n_jobs=-1,\n",
        "    random_state=RANDOM_STATE,\n",
        ")\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print('Best RMSE (log space):', -random_search.best_score_)\n",
        "print('Best params:')\n",
        "for k, v in random_search.best_params_.items():\n",
        "    print(f'  {k}: {v}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 13 · Evaluate on validation set\n",
        "Convert predictions back to the natural scale with `expm1` and compute RMSE / MAE for interpretability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 14. Score the tuned pipeline on the hold-out validation slice\n",
        "#     to confirm the search produced a stronger configuration.\n",
        "# ============================================================\n",
        "best_pipeline = random_search.best_estimator_\n",
        "val_pred_log = best_pipeline.predict(X_val)\n",
        "val_pred = np.expm1(val_pred_log)\n",
        "val_actual = np.expm1(y_val)\n",
        "\n",
        "val_rmse = mean_squared_error(val_actual, val_pred, squared=False)\n",
        "val_mae = mean_absolute_error(val_actual, val_pred)\n",
        "print(f'Validation RMSE: {val_rmse:,.2f}')\n",
        "print(f'Validation MAE:  {val_mae:,.2f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 14 · Inspect feature contributions\n",
        "A quick feature importance review spotlights which engineered fields drive the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 15. Combine the preprocessor's feature names with XGBoost's\n",
        "#     gain-based importances to see top drivers.\n",
        "# ============================================================\n",
        "feature_names = best_pipeline.named_steps['preprocess'].get_feature_names_out()\n",
        "importances = best_pipeline.named_steps['model'].feature_importances_\n",
        "importance_df = (\n",
        "    pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
        "    .sort_values('importance', ascending=False)\n",
        "    .head(20)\n",
        ")\n",
        "display(importance_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 15 · Refit on all available data\n",
        "Once validation looks good, retrain the tuned pipeline on the full training span (train + validation) to maximize signal before forecasting the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 16. Refit the best pipeline on the complete training window.\n",
        "#     This leverages every historical observation for final preds.\n",
        "# ============================================================\n",
        "X_full = train_ready.drop(columns=['Sales', 'Sales_Log', 'Date', 'Id'], errors='ignore')\n",
        "y_full = train_ready['Sales_Log']\n",
        "best_pipeline.fit(X_full, y_full)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 16 · Generate competition-style predictions\n",
        "Apply the final pipeline to the prepared test set, invert the log-transform, and package predictions with the `Id` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 17. Score the test rows and create a submission-ready frame.\n",
        "# ============================================================\n",
        "test_features = test_ready.drop(columns=['Sales', 'Sales_Log', 'Date'], errors='ignore')\n",
        "test_ids = test_ready['Id'] if 'Id' in test_ready.columns else test_df['Id']\n",
        "test_pred_log = best_pipeline.predict(test_features)\n",
        "test_predictions = np.expm1(test_pred_log)\n",
        "submission = pd.DataFrame({'Id': test_ids, 'Sales': test_predictions})\n",
        "display(submission.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 17 · Save for Kaggle submission (optional)\n",
        "Uncomment the final line to persist a CSV that you can submit to Kaggle for the Rossmann challenge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 18. Write predictions to disk if you want to submit results.\n",
        "# ============================================================\n",
        "# output_path = Path('rossmann_xgboost_submission.csv')\n",
        "# submission.to_csv(output_path, index=False)\n",
        "# print(f'Submission file saved to {output_path.resolve()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Where to go next\n",
        "- Compare `RandomizedSearchCV` with time-series aware CV (e.g., `TimeSeriesSplit`).\n",
        "- Engineer store-level aggregates (rolling means, promotion cadence).\n",
        "- Blend multiple models (e.g., LightGBM, CatBoost) for ensemble gains.\n",
        "- Track experiments with MLflow or Weights & Biases for reproducibility."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
