{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNMxiQKqalAZI3L/EClNB4l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"uZ5riZ4HkU4Q","executionInfo":{"status":"ok","timestamp":1760008456725,"user_tz":-120,"elapsed":10882,"user":{"displayName":"Oleksandr Altukhov","userId":"00390505112035995851"}}},"outputs":[],"source":["import torch\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class Standardizer(nn.Module):\n","    def __init__(self, eps=1e-8):\n","        super().__init__()\n","        self.register_buffer(\"mean\", None)\n","        self.register_buffer(\"std\", None)\n","        self.eps = eps\n","\n","    def fit(self, X):\n","        with torch.no_grad():\n","            self.mean = X.mean(dim=0)\n","            self.std  = X.std(dim=0).clamp_min(self.eps)\n","        return self\n","\n","    def forward(self, X):\n","        return (X - self.mean) / self.std\n","\n","    def inverse(self, Xn):\n","        return Xn * self.std + self.mean\n","\n","\n","\"\"\" ####Usage####\n","x_norm = Standardizer().fit(X_train)\n","y_norm = Standardizer().fit(y_train)\n","\n","Xtr = x_norm(X_train)\n","Xte = x_norm(X_test)\n","ytr = y_norm(y_train)\n","yte = y_norm(y_test)\n","\n","# At inference:\n","with torch.no_grad():\n","    y_pred = y_norm.inverse(model(Xte))\"\"\"\n"],"metadata":{"id":"xEaSzmfIu_BR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(42)\n","\n","n_samples = 1000\n","\n","# 3 features: size (m2), bedrooms, age\n","size = torch.randint(40, 2000, (n_samples, 1), dtype=torch.float32)\n","badrooms = torch.randint(1, 6, (n_samples, 1), dtype=torch.float32)\n","age = torch.randint(0, 50, (n_samples, 1), dtype=torch.float32)\n","\n","# Stack into feature matrix X (shape [1000, 3])\n","X = torch.cat([size, badrooms, age], dim=1)\n","\n","\n","# True weights for each feature (hidden truth)\n","true_weights = torch.tensor([[0.5],   # per m2\n","                             [20.0],  # per bedroom\n","                             [-0.3]]) # per year of age\n","\n","true_bias = 30.0  # base price (100 000 NOK)\n","# Generate target y = Xw + b + noise\n","noise = torch.randn(n_samples, 1) * 10\n","y = X @ true_weights + true_bias + noise\n","\n","x_scaler = StandardScaler()\n","y_scaler = StandardScaler()\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","Xtr = torch.from_numpy(x_scaler.fit_transform(X_train.numpy())).float().to(device)\n","Xte = torch.from_numpy(x_scaler.transform(X_test.numpy())).float().to(device)\n","\n","ytr = torch.from_numpy(y_scaler.fit_transform(y_train.numpy())).float().to(device)\n","yte = torch.from_numpy(y_scaler.transform(y_test.numpy())).float().to(device)"],"metadata":{"id":"w6ygOZDykrcm","executionInfo":{"status":"ok","timestamp":1760008493865,"user_tz":-120,"elapsed":269,"user":{"displayName":"Oleksandr Altukhov","userId":"00390505112035995851"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class LinRegModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.linear = nn.Linear(in_features=3,\n","                                out_features=1)\n","    def forward(self,x):\n","        return self.linear(x)"],"metadata":{"id":"M64kAcd2mgok","executionInfo":{"status":"ok","timestamp":1760008498103,"user_tz":-120,"elapsed":12,"user":{"displayName":"Oleksandr Altukhov","userId":"00390505112035995851"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#training loop\n","model = LinRegModel().to(device)\n","loss_fn = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n","\n","epochs = 100_000\n","for epoch in range(epochs):\n","    model.train()\n","\n","    y_pred = model(Xtr)\n","    loss = loss_fn(y_pred, ytr)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if epoch % 200 == 0:\n","        model.eval()\n","        with torch.no_grad():\n","            val_loss = loss_fn(model(Xte), yte)\n","        print(f\"Epoch {epoch}, Train loss: {loss.item():.2f}, Val loss: {val_loss.item():.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GT6X_syTnFmj","executionInfo":{"status":"ok","timestamp":1760008598716,"user_tz":-120,"elapsed":97823,"user":{"displayName":"Oleksandr Altukhov","userId":"00390505112035995851"}},"outputId":"1e43a16d-71ae-4665-8d61-cd0ed11abfe8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Train loss: 1.13, Val loss: 1.24\n","Epoch 200, Train loss: 1.12, Val loss: 1.24\n","Epoch 400, Train loss: 1.11, Val loss: 1.23\n","Epoch 600, Train loss: 1.10, Val loss: 1.22\n","Epoch 800, Train loss: 1.10, Val loss: 1.21\n","Epoch 1000, Train loss: 1.09, Val loss: 1.20\n","Epoch 1200, Train loss: 1.08, Val loss: 1.19\n","Epoch 1400, Train loss: 1.07, Val loss: 1.18\n","Epoch 1600, Train loss: 1.07, Val loss: 1.18\n","Epoch 1800, Train loss: 1.06, Val loss: 1.17\n","Epoch 2000, Train loss: 1.05, Val loss: 1.16\n","Epoch 2200, Train loss: 1.05, Val loss: 1.15\n","Epoch 2400, Train loss: 1.04, Val loss: 1.14\n","Epoch 2600, Train loss: 1.03, Val loss: 1.13\n","Epoch 2800, Train loss: 1.02, Val loss: 1.13\n","Epoch 3000, Train loss: 1.02, Val loss: 1.12\n","Epoch 3200, Train loss: 1.01, Val loss: 1.11\n","Epoch 3400, Train loss: 1.00, Val loss: 1.10\n","Epoch 3600, Train loss: 1.00, Val loss: 1.09\n","Epoch 3800, Train loss: 0.99, Val loss: 1.08\n","Epoch 4000, Train loss: 0.98, Val loss: 1.08\n","Epoch 4200, Train loss: 0.98, Val loss: 1.07\n","Epoch 4400, Train loss: 0.97, Val loss: 1.06\n","Epoch 4600, Train loss: 0.96, Val loss: 1.05\n","Epoch 4800, Train loss: 0.96, Val loss: 1.05\n","Epoch 5000, Train loss: 0.95, Val loss: 1.04\n","Epoch 5200, Train loss: 0.94, Val loss: 1.03\n","Epoch 5400, Train loss: 0.93, Val loss: 1.02\n","Epoch 5600, Train loss: 0.93, Val loss: 1.01\n","Epoch 5800, Train loss: 0.92, Val loss: 1.01\n","Epoch 6000, Train loss: 0.92, Val loss: 1.00\n","Epoch 6200, Train loss: 0.91, Val loss: 0.99\n","Epoch 6400, Train loss: 0.90, Val loss: 0.98\n","Epoch 6600, Train loss: 0.90, Val loss: 0.98\n","Epoch 6800, Train loss: 0.89, Val loss: 0.97\n","Epoch 7000, Train loss: 0.88, Val loss: 0.96\n","Epoch 7200, Train loss: 0.88, Val loss: 0.95\n","Epoch 7400, Train loss: 0.87, Val loss: 0.95\n","Epoch 7600, Train loss: 0.86, Val loss: 0.94\n","Epoch 7800, Train loss: 0.86, Val loss: 0.93\n","Epoch 8000, Train loss: 0.85, Val loss: 0.93\n","Epoch 8200, Train loss: 0.84, Val loss: 0.92\n","Epoch 8400, Train loss: 0.84, Val loss: 0.91\n","Epoch 8600, Train loss: 0.83, Val loss: 0.90\n","Epoch 8800, Train loss: 0.83, Val loss: 0.90\n","Epoch 9000, Train loss: 0.82, Val loss: 0.89\n","Epoch 9200, Train loss: 0.81, Val loss: 0.88\n","Epoch 9400, Train loss: 0.81, Val loss: 0.88\n","Epoch 9600, Train loss: 0.80, Val loss: 0.87\n","Epoch 9800, Train loss: 0.80, Val loss: 0.86\n","Epoch 10000, Train loss: 0.79, Val loss: 0.85\n","Epoch 10200, Train loss: 0.78, Val loss: 0.85\n","Epoch 10400, Train loss: 0.78, Val loss: 0.84\n","Epoch 10600, Train loss: 0.77, Val loss: 0.83\n","Epoch 10800, Train loss: 0.77, Val loss: 0.83\n","Epoch 11000, Train loss: 0.76, Val loss: 0.82\n","Epoch 11200, Train loss: 0.75, Val loss: 0.81\n","Epoch 11400, Train loss: 0.75, Val loss: 0.81\n","Epoch 11600, Train loss: 0.74, Val loss: 0.80\n","Epoch 11800, Train loss: 0.74, Val loss: 0.80\n","Epoch 12000, Train loss: 0.73, Val loss: 0.79\n","Epoch 12200, Train loss: 0.73, Val loss: 0.78\n","Epoch 12400, Train loss: 0.72, Val loss: 0.78\n","Epoch 12600, Train loss: 0.72, Val loss: 0.77\n","Epoch 12800, Train loss: 0.71, Val loss: 0.76\n","Epoch 13000, Train loss: 0.70, Val loss: 0.76\n","Epoch 13200, Train loss: 0.70, Val loss: 0.75\n","Epoch 13400, Train loss: 0.69, Val loss: 0.74\n","Epoch 13600, Train loss: 0.69, Val loss: 0.74\n","Epoch 13800, Train loss: 0.68, Val loss: 0.73\n","Epoch 14000, Train loss: 0.68, Val loss: 0.73\n","Epoch 14200, Train loss: 0.67, Val loss: 0.72\n","Epoch 14400, Train loss: 0.67, Val loss: 0.71\n","Epoch 14600, Train loss: 0.66, Val loss: 0.71\n","Epoch 14800, Train loss: 0.66, Val loss: 0.70\n","Epoch 15000, Train loss: 0.65, Val loss: 0.70\n","Epoch 15200, Train loss: 0.64, Val loss: 0.69\n","Epoch 15400, Train loss: 0.64, Val loss: 0.69\n","Epoch 15600, Train loss: 0.63, Val loss: 0.68\n","Epoch 15800, Train loss: 0.63, Val loss: 0.67\n","Epoch 16000, Train loss: 0.62, Val loss: 0.67\n","Epoch 16200, Train loss: 0.62, Val loss: 0.66\n","Epoch 16400, Train loss: 0.61, Val loss: 0.66\n","Epoch 16600, Train loss: 0.61, Val loss: 0.65\n","Epoch 16800, Train loss: 0.60, Val loss: 0.65\n","Epoch 17000, Train loss: 0.60, Val loss: 0.64\n","Epoch 17200, Train loss: 0.59, Val loss: 0.64\n","Epoch 17400, Train loss: 0.59, Val loss: 0.63\n","Epoch 17600, Train loss: 0.58, Val loss: 0.63\n","Epoch 17800, Train loss: 0.58, Val loss: 0.62\n","Epoch 18000, Train loss: 0.57, Val loss: 0.61\n","Epoch 18200, Train loss: 0.57, Val loss: 0.61\n","Epoch 18400, Train loss: 0.56, Val loss: 0.60\n","Epoch 18600, Train loss: 0.56, Val loss: 0.60\n","Epoch 18800, Train loss: 0.55, Val loss: 0.59\n","Epoch 19000, Train loss: 0.55, Val loss: 0.59\n","Epoch 19200, Train loss: 0.55, Val loss: 0.58\n","Epoch 19400, Train loss: 0.54, Val loss: 0.58\n","Epoch 19600, Train loss: 0.54, Val loss: 0.57\n","Epoch 19800, Train loss: 0.53, Val loss: 0.57\n","Epoch 20000, Train loss: 0.53, Val loss: 0.56\n","Epoch 20200, Train loss: 0.52, Val loss: 0.56\n","Epoch 20400, Train loss: 0.52, Val loss: 0.55\n","Epoch 20600, Train loss: 0.51, Val loss: 0.55\n","Epoch 20800, Train loss: 0.51, Val loss: 0.54\n","Epoch 21000, Train loss: 0.50, Val loss: 0.54\n","Epoch 21200, Train loss: 0.50, Val loss: 0.53\n","Epoch 21400, Train loss: 0.49, Val loss: 0.53\n","Epoch 21600, Train loss: 0.49, Val loss: 0.52\n","Epoch 21800, Train loss: 0.49, Val loss: 0.52\n","Epoch 22000, Train loss: 0.48, Val loss: 0.52\n","Epoch 22200, Train loss: 0.48, Val loss: 0.51\n","Epoch 22400, Train loss: 0.47, Val loss: 0.51\n","Epoch 22600, Train loss: 0.47, Val loss: 0.50\n","Epoch 22800, Train loss: 0.46, Val loss: 0.50\n","Epoch 23000, Train loss: 0.46, Val loss: 0.49\n","Epoch 23200, Train loss: 0.46, Val loss: 0.49\n","Epoch 23400, Train loss: 0.45, Val loss: 0.48\n","Epoch 23600, Train loss: 0.45, Val loss: 0.48\n","Epoch 23800, Train loss: 0.44, Val loss: 0.47\n","Epoch 24000, Train loss: 0.44, Val loss: 0.47\n","Epoch 24200, Train loss: 0.43, Val loss: 0.46\n","Epoch 24400, Train loss: 0.43, Val loss: 0.46\n","Epoch 24600, Train loss: 0.43, Val loss: 0.46\n","Epoch 24800, Train loss: 0.42, Val loss: 0.45\n","Epoch 25000, Train loss: 0.42, Val loss: 0.45\n","Epoch 25200, Train loss: 0.41, Val loss: 0.44\n","Epoch 25400, Train loss: 0.41, Val loss: 0.44\n","Epoch 25600, Train loss: 0.41, Val loss: 0.43\n","Epoch 25800, Train loss: 0.40, Val loss: 0.43\n","Epoch 26000, Train loss: 0.40, Val loss: 0.43\n","Epoch 26200, Train loss: 0.39, Val loss: 0.42\n","Epoch 26400, Train loss: 0.39, Val loss: 0.42\n","Epoch 26600, Train loss: 0.39, Val loss: 0.41\n","Epoch 26800, Train loss: 0.38, Val loss: 0.41\n","Epoch 27000, Train loss: 0.38, Val loss: 0.41\n","Epoch 27200, Train loss: 0.37, Val loss: 0.40\n","Epoch 27400, Train loss: 0.37, Val loss: 0.40\n","Epoch 27600, Train loss: 0.37, Val loss: 0.39\n","Epoch 27800, Train loss: 0.36, Val loss: 0.39\n","Epoch 28000, Train loss: 0.36, Val loss: 0.39\n","Epoch 28200, Train loss: 0.36, Val loss: 0.38\n","Epoch 28400, Train loss: 0.35, Val loss: 0.38\n","Epoch 28600, Train loss: 0.35, Val loss: 0.37\n","Epoch 28800, Train loss: 0.35, Val loss: 0.37\n","Epoch 29000, Train loss: 0.34, Val loss: 0.37\n","Epoch 29200, Train loss: 0.34, Val loss: 0.36\n","Epoch 29400, Train loss: 0.33, Val loss: 0.36\n","Epoch 29600, Train loss: 0.33, Val loss: 0.35\n","Epoch 29800, Train loss: 0.33, Val loss: 0.35\n","Epoch 30000, Train loss: 0.32, Val loss: 0.35\n","Epoch 30200, Train loss: 0.32, Val loss: 0.34\n","Epoch 30400, Train loss: 0.32, Val loss: 0.34\n","Epoch 30600, Train loss: 0.31, Val loss: 0.34\n","Epoch 30800, Train loss: 0.31, Val loss: 0.33\n","Epoch 31000, Train loss: 0.31, Val loss: 0.33\n","Epoch 31200, Train loss: 0.30, Val loss: 0.32\n","Epoch 31400, Train loss: 0.30, Val loss: 0.32\n","Epoch 31600, Train loss: 0.30, Val loss: 0.32\n","Epoch 31800, Train loss: 0.29, Val loss: 0.31\n","Epoch 32000, Train loss: 0.29, Val loss: 0.31\n","Epoch 32200, Train loss: 0.29, Val loss: 0.31\n","Epoch 32400, Train loss: 0.28, Val loss: 0.30\n","Epoch 32600, Train loss: 0.28, Val loss: 0.30\n","Epoch 32800, Train loss: 0.28, Val loss: 0.30\n","Epoch 33000, Train loss: 0.27, Val loss: 0.29\n","Epoch 33200, Train loss: 0.27, Val loss: 0.29\n","Epoch 33400, Train loss: 0.27, Val loss: 0.29\n","Epoch 33600, Train loss: 0.27, Val loss: 0.28\n","Epoch 33800, Train loss: 0.26, Val loss: 0.28\n","Epoch 34000, Train loss: 0.26, Val loss: 0.28\n","Epoch 34200, Train loss: 0.26, Val loss: 0.27\n","Epoch 34400, Train loss: 0.25, Val loss: 0.27\n","Epoch 34600, Train loss: 0.25, Val loss: 0.27\n","Epoch 34800, Train loss: 0.25, Val loss: 0.26\n","Epoch 35000, Train loss: 0.24, Val loss: 0.26\n","Epoch 35200, Train loss: 0.24, Val loss: 0.26\n","Epoch 35400, Train loss: 0.24, Val loss: 0.26\n","Epoch 35600, Train loss: 0.24, Val loss: 0.25\n","Epoch 35800, Train loss: 0.23, Val loss: 0.25\n","Epoch 36000, Train loss: 0.23, Val loss: 0.25\n","Epoch 36200, Train loss: 0.23, Val loss: 0.24\n","Epoch 36400, Train loss: 0.22, Val loss: 0.24\n","Epoch 36600, Train loss: 0.22, Val loss: 0.24\n","Epoch 36800, Train loss: 0.22, Val loss: 0.23\n","Epoch 37000, Train loss: 0.22, Val loss: 0.23\n","Epoch 37200, Train loss: 0.21, Val loss: 0.23\n","Epoch 37400, Train loss: 0.21, Val loss: 0.23\n","Epoch 37600, Train loss: 0.21, Val loss: 0.22\n","Epoch 37800, Train loss: 0.21, Val loss: 0.22\n","Epoch 38000, Train loss: 0.20, Val loss: 0.22\n","Epoch 38200, Train loss: 0.20, Val loss: 0.21\n","Epoch 38400, Train loss: 0.20, Val loss: 0.21\n","Epoch 38600, Train loss: 0.20, Val loss: 0.21\n","Epoch 38800, Train loss: 0.19, Val loss: 0.21\n","Epoch 39000, Train loss: 0.19, Val loss: 0.20\n","Epoch 39200, Train loss: 0.19, Val loss: 0.20\n","Epoch 39400, Train loss: 0.19, Val loss: 0.20\n","Epoch 39600, Train loss: 0.18, Val loss: 0.20\n","Epoch 39800, Train loss: 0.18, Val loss: 0.19\n","Epoch 40000, Train loss: 0.18, Val loss: 0.19\n","Epoch 40200, Train loss: 0.18, Val loss: 0.19\n","Epoch 40400, Train loss: 0.17, Val loss: 0.19\n","Epoch 40600, Train loss: 0.17, Val loss: 0.18\n","Epoch 40800, Train loss: 0.17, Val loss: 0.18\n","Epoch 41000, Train loss: 0.17, Val loss: 0.18\n","Epoch 41200, Train loss: 0.16, Val loss: 0.18\n","Epoch 41400, Train loss: 0.16, Val loss: 0.17\n","Epoch 41600, Train loss: 0.16, Val loss: 0.17\n","Epoch 41800, Train loss: 0.16, Val loss: 0.17\n","Epoch 42000, Train loss: 0.16, Val loss: 0.17\n","Epoch 42200, Train loss: 0.15, Val loss: 0.16\n","Epoch 42400, Train loss: 0.15, Val loss: 0.16\n","Epoch 42600, Train loss: 0.15, Val loss: 0.16\n","Epoch 42800, Train loss: 0.15, Val loss: 0.16\n","Epoch 43000, Train loss: 0.15, Val loss: 0.15\n","Epoch 43200, Train loss: 0.14, Val loss: 0.15\n","Epoch 43400, Train loss: 0.14, Val loss: 0.15\n","Epoch 43600, Train loss: 0.14, Val loss: 0.15\n","Epoch 43800, Train loss: 0.14, Val loss: 0.15\n","Epoch 44000, Train loss: 0.13, Val loss: 0.14\n","Epoch 44200, Train loss: 0.13, Val loss: 0.14\n","Epoch 44400, Train loss: 0.13, Val loss: 0.14\n","Epoch 44600, Train loss: 0.13, Val loss: 0.14\n","Epoch 44800, Train loss: 0.13, Val loss: 0.14\n","Epoch 45000, Train loss: 0.12, Val loss: 0.13\n","Epoch 45200, Train loss: 0.12, Val loss: 0.13\n","Epoch 45400, Train loss: 0.12, Val loss: 0.13\n","Epoch 45600, Train loss: 0.12, Val loss: 0.13\n","Epoch 45800, Train loss: 0.12, Val loss: 0.13\n","Epoch 46000, Train loss: 0.12, Val loss: 0.12\n","Epoch 46200, Train loss: 0.11, Val loss: 0.12\n","Epoch 46400, Train loss: 0.11, Val loss: 0.12\n","Epoch 46600, Train loss: 0.11, Val loss: 0.12\n","Epoch 46800, Train loss: 0.11, Val loss: 0.12\n","Epoch 47000, Train loss: 0.11, Val loss: 0.11\n","Epoch 47200, Train loss: 0.10, Val loss: 0.11\n","Epoch 47400, Train loss: 0.10, Val loss: 0.11\n","Epoch 47600, Train loss: 0.10, Val loss: 0.11\n","Epoch 47800, Train loss: 0.10, Val loss: 0.11\n","Epoch 48000, Train loss: 0.10, Val loss: 0.10\n","Epoch 48200, Train loss: 0.10, Val loss: 0.10\n","Epoch 48400, Train loss: 0.09, Val loss: 0.10\n","Epoch 48600, Train loss: 0.09, Val loss: 0.10\n","Epoch 48800, Train loss: 0.09, Val loss: 0.10\n","Epoch 49000, Train loss: 0.09, Val loss: 0.10\n","Epoch 49200, Train loss: 0.09, Val loss: 0.09\n","Epoch 49400, Train loss: 0.09, Val loss: 0.09\n","Epoch 49600, Train loss: 0.08, Val loss: 0.09\n","Epoch 49800, Train loss: 0.08, Val loss: 0.09\n","Epoch 50000, Train loss: 0.08, Val loss: 0.09\n","Epoch 50200, Train loss: 0.08, Val loss: 0.09\n","Epoch 50400, Train loss: 0.08, Val loss: 0.08\n","Epoch 50600, Train loss: 0.08, Val loss: 0.08\n","Epoch 50800, Train loss: 0.08, Val loss: 0.08\n","Epoch 51000, Train loss: 0.07, Val loss: 0.08\n","Epoch 51200, Train loss: 0.07, Val loss: 0.08\n","Epoch 51400, Train loss: 0.07, Val loss: 0.08\n","Epoch 51600, Train loss: 0.07, Val loss: 0.07\n","Epoch 51800, Train loss: 0.07, Val loss: 0.07\n","Epoch 52000, Train loss: 0.07, Val loss: 0.07\n","Epoch 52200, Train loss: 0.07, Val loss: 0.07\n","Epoch 52400, Train loss: 0.06, Val loss: 0.07\n","Epoch 52600, Train loss: 0.06, Val loss: 0.07\n","Epoch 52800, Train loss: 0.06, Val loss: 0.07\n","Epoch 53000, Train loss: 0.06, Val loss: 0.06\n","Epoch 53200, Train loss: 0.06, Val loss: 0.06\n","Epoch 53400, Train loss: 0.06, Val loss: 0.06\n","Epoch 53600, Train loss: 0.06, Val loss: 0.06\n","Epoch 53800, Train loss: 0.05, Val loss: 0.06\n","Epoch 54000, Train loss: 0.05, Val loss: 0.06\n","Epoch 54200, Train loss: 0.05, Val loss: 0.06\n","Epoch 54400, Train loss: 0.05, Val loss: 0.06\n","Epoch 54600, Train loss: 0.05, Val loss: 0.05\n","Epoch 54800, Train loss: 0.05, Val loss: 0.05\n","Epoch 55000, Train loss: 0.05, Val loss: 0.05\n","Epoch 55200, Train loss: 0.05, Val loss: 0.05\n","Epoch 55400, Train loss: 0.05, Val loss: 0.05\n","Epoch 55600, Train loss: 0.04, Val loss: 0.05\n","Epoch 55800, Train loss: 0.04, Val loss: 0.05\n","Epoch 56000, Train loss: 0.04, Val loss: 0.05\n","Epoch 56200, Train loss: 0.04, Val loss: 0.04\n","Epoch 56400, Train loss: 0.04, Val loss: 0.04\n","Epoch 56600, Train loss: 0.04, Val loss: 0.04\n","Epoch 56800, Train loss: 0.04, Val loss: 0.04\n","Epoch 57000, Train loss: 0.04, Val loss: 0.04\n","Epoch 57200, Train loss: 0.04, Val loss: 0.04\n","Epoch 57400, Train loss: 0.03, Val loss: 0.04\n","Epoch 57600, Train loss: 0.03, Val loss: 0.04\n","Epoch 57800, Train loss: 0.03, Val loss: 0.04\n","Epoch 58000, Train loss: 0.03, Val loss: 0.03\n","Epoch 58200, Train loss: 0.03, Val loss: 0.03\n","Epoch 58400, Train loss: 0.03, Val loss: 0.03\n","Epoch 58600, Train loss: 0.03, Val loss: 0.03\n","Epoch 58800, Train loss: 0.03, Val loss: 0.03\n","Epoch 59000, Train loss: 0.03, Val loss: 0.03\n","Epoch 59200, Train loss: 0.03, Val loss: 0.03\n","Epoch 59400, Train loss: 0.03, Val loss: 0.03\n","Epoch 59600, Train loss: 0.03, Val loss: 0.03\n","Epoch 59800, Train loss: 0.02, Val loss: 0.03\n","Epoch 60000, Train loss: 0.02, Val loss: 0.03\n","Epoch 60200, Train loss: 0.02, Val loss: 0.03\n","Epoch 60400, Train loss: 0.02, Val loss: 0.02\n","Epoch 60600, Train loss: 0.02, Val loss: 0.02\n","Epoch 60800, Train loss: 0.02, Val loss: 0.02\n","Epoch 61000, Train loss: 0.02, Val loss: 0.02\n","Epoch 61200, Train loss: 0.02, Val loss: 0.02\n","Epoch 61400, Train loss: 0.02, Val loss: 0.02\n","Epoch 61600, Train loss: 0.02, Val loss: 0.02\n","Epoch 61800, Train loss: 0.02, Val loss: 0.02\n","Epoch 62000, Train loss: 0.02, Val loss: 0.02\n","Epoch 62200, Train loss: 0.02, Val loss: 0.02\n","Epoch 62400, Train loss: 0.02, Val loss: 0.02\n","Epoch 62600, Train loss: 0.02, Val loss: 0.02\n","Epoch 62800, Train loss: 0.01, Val loss: 0.02\n","Epoch 63000, Train loss: 0.01, Val loss: 0.02\n","Epoch 63200, Train loss: 0.01, Val loss: 0.02\n","Epoch 63400, Train loss: 0.01, Val loss: 0.01\n","Epoch 63600, Train loss: 0.01, Val loss: 0.01\n","Epoch 63800, Train loss: 0.01, Val loss: 0.01\n","Epoch 64000, Train loss: 0.01, Val loss: 0.01\n","Epoch 64200, Train loss: 0.01, Val loss: 0.01\n","Epoch 64400, Train loss: 0.01, Val loss: 0.01\n","Epoch 64600, Train loss: 0.01, Val loss: 0.01\n","Epoch 64800, Train loss: 0.01, Val loss: 0.01\n","Epoch 65000, Train loss: 0.01, Val loss: 0.01\n","Epoch 65200, Train loss: 0.01, Val loss: 0.01\n","Epoch 65400, Train loss: 0.01, Val loss: 0.01\n","Epoch 65600, Train loss: 0.01, Val loss: 0.01\n","Epoch 65800, Train loss: 0.01, Val loss: 0.01\n","Epoch 66000, Train loss: 0.01, Val loss: 0.01\n","Epoch 66200, Train loss: 0.01, Val loss: 0.01\n","Epoch 66400, Train loss: 0.01, Val loss: 0.01\n","Epoch 66600, Train loss: 0.01, Val loss: 0.01\n","Epoch 66800, Train loss: 0.01, Val loss: 0.01\n","Epoch 67000, Train loss: 0.01, Val loss: 0.01\n","Epoch 67200, Train loss: 0.01, Val loss: 0.01\n","Epoch 67400, Train loss: 0.01, Val loss: 0.01\n","Epoch 67600, Train loss: 0.01, Val loss: 0.01\n","Epoch 67800, Train loss: 0.01, Val loss: 0.01\n","Epoch 68000, Train loss: 0.01, Val loss: 0.01\n","Epoch 68200, Train loss: 0.00, Val loss: 0.01\n","Epoch 68400, Train loss: 0.00, Val loss: 0.01\n","Epoch 68600, Train loss: 0.00, Val loss: 0.00\n","Epoch 68800, Train loss: 0.00, Val loss: 0.00\n","Epoch 69000, Train loss: 0.00, Val loss: 0.00\n","Epoch 69200, Train loss: 0.00, Val loss: 0.00\n","Epoch 69400, Train loss: 0.00, Val loss: 0.00\n","Epoch 69600, Train loss: 0.00, Val loss: 0.00\n","Epoch 69800, Train loss: 0.00, Val loss: 0.00\n","Epoch 70000, Train loss: 0.00, Val loss: 0.00\n","Epoch 70200, Train loss: 0.00, Val loss: 0.00\n","Epoch 70400, Train loss: 0.00, Val loss: 0.00\n","Epoch 70600, Train loss: 0.00, Val loss: 0.00\n","Epoch 70800, Train loss: 0.00, Val loss: 0.00\n","Epoch 71000, Train loss: 0.00, Val loss: 0.00\n","Epoch 71200, Train loss: 0.00, Val loss: 0.00\n","Epoch 71400, Train loss: 0.00, Val loss: 0.00\n","Epoch 71600, Train loss: 0.00, Val loss: 0.00\n","Epoch 71800, Train loss: 0.00, Val loss: 0.00\n","Epoch 72000, Train loss: 0.00, Val loss: 0.00\n","Epoch 72200, Train loss: 0.00, Val loss: 0.00\n","Epoch 72400, Train loss: 0.00, Val loss: 0.00\n","Epoch 72600, Train loss: 0.00, Val loss: 0.00\n","Epoch 72800, Train loss: 0.00, Val loss: 0.00\n","Epoch 73000, Train loss: 0.00, Val loss: 0.00\n","Epoch 73200, Train loss: 0.00, Val loss: 0.00\n","Epoch 73400, Train loss: 0.00, Val loss: 0.00\n","Epoch 73600, Train loss: 0.00, Val loss: 0.00\n","Epoch 73800, Train loss: 0.00, Val loss: 0.00\n","Epoch 74000, Train loss: 0.00, Val loss: 0.00\n","Epoch 74200, Train loss: 0.00, Val loss: 0.00\n","Epoch 74400, Train loss: 0.00, Val loss: 0.00\n","Epoch 74600, Train loss: 0.00, Val loss: 0.00\n","Epoch 74800, Train loss: 0.00, Val loss: 0.00\n","Epoch 75000, Train loss: 0.00, Val loss: 0.00\n","Epoch 75200, Train loss: 0.00, Val loss: 0.00\n","Epoch 75400, Train loss: 0.00, Val loss: 0.00\n","Epoch 75600, Train loss: 0.00, Val loss: 0.00\n","Epoch 75800, Train loss: 0.00, Val loss: 0.00\n","Epoch 76000, Train loss: 0.00, Val loss: 0.00\n","Epoch 76200, Train loss: 0.00, Val loss: 0.00\n","Epoch 76400, Train loss: 0.00, Val loss: 0.00\n","Epoch 76600, Train loss: 0.00, Val loss: 0.00\n","Epoch 76800, Train loss: 0.00, Val loss: 0.00\n","Epoch 77000, Train loss: 0.00, Val loss: 0.00\n","Epoch 77200, Train loss: 0.00, Val loss: 0.00\n","Epoch 77400, Train loss: 0.00, Val loss: 0.00\n","Epoch 77600, Train loss: 0.00, Val loss: 0.00\n","Epoch 77800, Train loss: 0.00, Val loss: 0.00\n","Epoch 78000, Train loss: 0.00, Val loss: 0.00\n","Epoch 78200, Train loss: 0.00, Val loss: 0.00\n","Epoch 78400, Train loss: 0.00, Val loss: 0.00\n","Epoch 78600, Train loss: 0.00, Val loss: 0.00\n","Epoch 78800, Train loss: 0.00, Val loss: 0.00\n","Epoch 79000, Train loss: 0.00, Val loss: 0.00\n","Epoch 79200, Train loss: 0.00, Val loss: 0.00\n","Epoch 79400, Train loss: 0.00, Val loss: 0.00\n","Epoch 79600, Train loss: 0.00, Val loss: 0.00\n","Epoch 79800, Train loss: 0.00, Val loss: 0.00\n","Epoch 80000, Train loss: 0.00, Val loss: 0.00\n","Epoch 80200, Train loss: 0.00, Val loss: 0.00\n","Epoch 80400, Train loss: 0.00, Val loss: 0.00\n","Epoch 80600, Train loss: 0.00, Val loss: 0.00\n","Epoch 80800, Train loss: 0.00, Val loss: 0.00\n","Epoch 81000, Train loss: 0.00, Val loss: 0.00\n","Epoch 81200, Train loss: 0.00, Val loss: 0.00\n","Epoch 81400, Train loss: 0.00, Val loss: 0.00\n","Epoch 81600, Train loss: 0.00, Val loss: 0.00\n","Epoch 81800, Train loss: 0.00, Val loss: 0.00\n","Epoch 82000, Train loss: 0.00, Val loss: 0.00\n","Epoch 82200, Train loss: 0.00, Val loss: 0.00\n","Epoch 82400, Train loss: 0.00, Val loss: 0.00\n","Epoch 82600, Train loss: 0.00, Val loss: 0.00\n","Epoch 82800, Train loss: 0.00, Val loss: 0.00\n","Epoch 83000, Train loss: 0.00, Val loss: 0.00\n","Epoch 83200, Train loss: 0.00, Val loss: 0.00\n","Epoch 83400, Train loss: 0.00, Val loss: 0.00\n","Epoch 83600, Train loss: 0.00, Val loss: 0.00\n","Epoch 83800, Train loss: 0.00, Val loss: 0.00\n","Epoch 84000, Train loss: 0.00, Val loss: 0.00\n","Epoch 84200, Train loss: 0.00, Val loss: 0.00\n","Epoch 84400, Train loss: 0.00, Val loss: 0.00\n","Epoch 84600, Train loss: 0.00, Val loss: 0.00\n","Epoch 84800, Train loss: 0.00, Val loss: 0.00\n","Epoch 85000, Train loss: 0.00, Val loss: 0.00\n","Epoch 85200, Train loss: 0.00, Val loss: 0.00\n","Epoch 85400, Train loss: 0.00, Val loss: 0.00\n","Epoch 85600, Train loss: 0.00, Val loss: 0.00\n","Epoch 85800, Train loss: 0.00, Val loss: 0.00\n","Epoch 86000, Train loss: 0.00, Val loss: 0.00\n","Epoch 86200, Train loss: 0.00, Val loss: 0.00\n","Epoch 86400, Train loss: 0.00, Val loss: 0.00\n","Epoch 86600, Train loss: 0.00, Val loss: 0.00\n","Epoch 86800, Train loss: 0.00, Val loss: 0.00\n","Epoch 87000, Train loss: 0.00, Val loss: 0.00\n","Epoch 87200, Train loss: 0.00, Val loss: 0.00\n","Epoch 87400, Train loss: 0.00, Val loss: 0.00\n","Epoch 87600, Train loss: 0.00, Val loss: 0.00\n","Epoch 87800, Train loss: 0.00, Val loss: 0.00\n","Epoch 88000, Train loss: 0.00, Val loss: 0.00\n","Epoch 88200, Train loss: 0.00, Val loss: 0.00\n","Epoch 88400, Train loss: 0.00, Val loss: 0.00\n","Epoch 88600, Train loss: 0.00, Val loss: 0.00\n","Epoch 88800, Train loss: 0.00, Val loss: 0.00\n","Epoch 89000, Train loss: 0.00, Val loss: 0.00\n","Epoch 89200, Train loss: 0.00, Val loss: 0.00\n","Epoch 89400, Train loss: 0.00, Val loss: 0.00\n","Epoch 89600, Train loss: 0.00, Val loss: 0.00\n","Epoch 89800, Train loss: 0.00, Val loss: 0.00\n","Epoch 90000, Train loss: 0.00, Val loss: 0.00\n","Epoch 90200, Train loss: 0.00, Val loss: 0.00\n","Epoch 90400, Train loss: 0.00, Val loss: 0.00\n","Epoch 90600, Train loss: 0.00, Val loss: 0.00\n","Epoch 90800, Train loss: 0.00, Val loss: 0.00\n","Epoch 91000, Train loss: 0.00, Val loss: 0.00\n","Epoch 91200, Train loss: 0.00, Val loss: 0.00\n","Epoch 91400, Train loss: 0.00, Val loss: 0.00\n","Epoch 91600, Train loss: 0.00, Val loss: 0.00\n","Epoch 91800, Train loss: 0.00, Val loss: 0.00\n","Epoch 92000, Train loss: 0.00, Val loss: 0.00\n","Epoch 92200, Train loss: 0.00, Val loss: 0.00\n","Epoch 92400, Train loss: 0.00, Val loss: 0.00\n","Epoch 92600, Train loss: 0.00, Val loss: 0.00\n","Epoch 92800, Train loss: 0.00, Val loss: 0.00\n","Epoch 93000, Train loss: 0.00, Val loss: 0.00\n","Epoch 93200, Train loss: 0.00, Val loss: 0.00\n","Epoch 93400, Train loss: 0.00, Val loss: 0.00\n","Epoch 93600, Train loss: 0.00, Val loss: 0.00\n","Epoch 93800, Train loss: 0.00, Val loss: 0.00\n","Epoch 94000, Train loss: 0.00, Val loss: 0.00\n","Epoch 94200, Train loss: 0.00, Val loss: 0.00\n","Epoch 94400, Train loss: 0.00, Val loss: 0.00\n","Epoch 94600, Train loss: 0.00, Val loss: 0.00\n","Epoch 94800, Train loss: 0.00, Val loss: 0.00\n","Epoch 95000, Train loss: 0.00, Val loss: 0.00\n","Epoch 95200, Train loss: 0.00, Val loss: 0.00\n","Epoch 95400, Train loss: 0.00, Val loss: 0.00\n","Epoch 95600, Train loss: 0.00, Val loss: 0.00\n","Epoch 95800, Train loss: 0.00, Val loss: 0.00\n","Epoch 96000, Train loss: 0.00, Val loss: 0.00\n","Epoch 96200, Train loss: 0.00, Val loss: 0.00\n","Epoch 96400, Train loss: 0.00, Val loss: 0.00\n","Epoch 96600, Train loss: 0.00, Val loss: 0.00\n","Epoch 96800, Train loss: 0.00, Val loss: 0.00\n","Epoch 97000, Train loss: 0.00, Val loss: 0.00\n","Epoch 97200, Train loss: 0.00, Val loss: 0.00\n","Epoch 97400, Train loss: 0.00, Val loss: 0.00\n","Epoch 97600, Train loss: 0.00, Val loss: 0.00\n","Epoch 97800, Train loss: 0.00, Val loss: 0.00\n","Epoch 98000, Train loss: 0.00, Val loss: 0.00\n","Epoch 98200, Train loss: 0.00, Val loss: 0.00\n","Epoch 98400, Train loss: 0.00, Val loss: 0.00\n","Epoch 98600, Train loss: 0.00, Val loss: 0.00\n","Epoch 98800, Train loss: 0.00, Val loss: 0.00\n","Epoch 99000, Train loss: 0.00, Val loss: 0.00\n","Epoch 99200, Train loss: 0.00, Val loss: 0.00\n","Epoch 99400, Train loss: 0.00, Val loss: 0.00\n","Epoch 99600, Train loss: 0.00, Val loss: 0.00\n","Epoch 99800, Train loss: 0.00, Val loss: 0.00\n"]}]},{"cell_type":"code","source":["print(f\"Learned weights: {model.linear.weight.data} | True values: {true_weights}\")\n","print(f\"Learned bias: {model.linear.bias.data} | True value: {true_bias}\" )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"np9835OOodIm","executionInfo":{"status":"ok","timestamp":1760008606091,"user_tz":-120,"elapsed":331,"user":{"displayName":"Oleksandr Altukhov","userId":"00390505112035995851"}},"outputId":"dd0914d7-3d93-4445-9ea8-c3274867c417"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Learned weights: tensor([[ 0.9884,  0.0990, -0.0150]], device='cuda:0') | True values: tensor([[ 0.5000],\n","        [20.0000],\n","        [-0.3000]])\n","Learned bias: tensor([-1.0901e-09], device='cuda:0') | True value: 30.0\n"]}]},{"cell_type":"code","source":["# pull scaler stats (NumPy) and convert to torch on CPU\n","mx = torch.tensor(x_scaler.mean_,  dtype=torch.float32)      # [3]\n","sx = torch.tensor(x_scaler.scale_, dtype=torch.float32).clamp_min(1e-8)  # [3]\n","my = torch.tensor(y_scaler.mean_,  dtype=torch.float32)      # [1]\n","sy = torch.tensor(y_scaler.scale_, dtype=torch.float32).clamp_min(1e-8)  # [1]\n","\n","# trained (normalized-space) params\n","Wn = model.linear.weight.detach().cpu().squeeze(0)   # [3]\n","bn = model.linear.bias.detach().cpu().squeeze(0)     # []\n","\n","# convert to original scale\n","W_orig = sy.squeeze(0) * (Wn / sx)                                # [3]\n","b_orig = sy.squeeze(0) * (bn - (mx / sx) @ Wn) + my.squeeze(0)    # []\n","\n","print(\"True   W:\", true_weights.squeeze().tolist(), \" b:\", float(true_bias))\n","print(\"Learned W:\", W_orig.tolist(), \" b:\", float(b_orig))\n","\n","# evaluate MSE in original scale\n","with torch.no_grad():\n","    yhat_test_orig = torch.from_numpy(\n","        y_scaler.inverse_transform(model(Xte).detach().cpu().numpy())\n","    )\n","    mse_orig = ((yhat_test_orig - y_test.cpu())**2).mean().item()\n","print(f\"MSE (original scale): {mse_orig:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oNAOpZEZodok","executionInfo":{"status":"ok","timestamp":1760008874351,"user_tz":-120,"elapsed":21,"user":{"displayName":"Oleksandr Altukhov","userId":"00390505112035995851"}},"outputId":"f0d1b943-554f-4837-f088-67430d17e5ae"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["True   W: [0.5, 20.0, -0.30000001192092896]  b: 30.0\n","Learned W: [0.500735878944397, 19.950336456298828, -0.29907187819480896]  b: 29.71417236328125\n","MSE (original scale): 100.692\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"GNhgUk_vu9H3"},"execution_count":null,"outputs":[]}]}